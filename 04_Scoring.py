# Databricks notebook source
# MAGIC %md The purpose of this notebook is to score households for product propensity. 

# COMMAND ----------

# MAGIC %md ## Introduction
# MAGIC 
# MAGIC With our features generated and our model trained, we can now turn our attention to the scoring of individual households for their propensity to buy from a commodity category.  Because we have registered our model with feature store information, this process will be greatly simplified compared to other approaches.

# COMMAND ----------

# DBTITLE 1,Retrieve Configuration Values
# MAGIC %run "./00_Overview & Configuration"

# COMMAND ----------

# DBTITLE 1,Import Required Libraries
import mlflow
from delta.tables import *
from databricks.feature_store import feature_table, FeatureStoreClient

import pyspark.sql.functions as f

import pandas as pd

# COMMAND ----------

# DBTITLE 1,Set Current Database
spark.catalog.setCurrentDatabase(config['database'])

# COMMAND ----------

# MAGIC %md ## Step 1: Assemble Set to Score
# MAGIC 
# MAGIC If we were performing batch inference (scoring) as part of a daily cycle, we'd know the day for which we have features available to us. Because we are working with a demonstration dataset that stops at some point in the past, we will need to look at our data to see what the last day available to us is.  We could do this by querying the transactional data as follows:

# COMMAND ----------

# DBTITLE 1,Get Transactions
transactions = spark.table('transactions_adj')

# COMMAND ----------

# DBTITLE 1,Get Day
last_day = (
  transactions
    .groupBy()
      .agg(f.max('day').alias('last_day')) # get last day in set
    .collect()
  )[0]['last_day']

# COMMAND ----------

# MAGIC %md As explained in the last notebook, we elected to train one model for each commodity of interest.  To identify commodity for which we trained the model, we employed a widget which we will define again here.  It's important this widget be set to the same value as was used to train the model we intend to retrieve:

# COMMAND ----------

# DBTITLE 1,Create Widget Parameter
# MAGIC %sql
# MAGIC 
# MAGIC CREATE WIDGET DROPDOWN Commodity DEFAULT "SOFT DRINKS" 
# MAGIC CHOICES 
# MAGIC   SELECT 
# MAGIC     commodity_desc
# MAGIC   FROM 
# MAGIC   (
# MAGIC     SELECT 
# MAGIC       commodity_desc, 
# MAGIC       count(commodity_desc) as count
# MAGIC     FROM products
# MAGIC     GROUP BY commodity_desc
# MAGIC     ORDER BY count desc
# MAGIC   )

# COMMAND ----------

# MAGIC %md **Note**: We will access the widget value in the same language it was defined in to avoid any confusion by the runtime for `Run All` and `Workflow` executions.

# COMMAND ----------

# DBTITLE 1,Get Commodity
commodity_desc= spark.sql("select getArgument('Commodity')").take(1)[0][0]
commodity_desc

# COMMAND ----------

# MAGIC %md And finally, we need to get a list of households we wish to score:

# COMMAND ----------

# DBTITLE 1,Get Households
households = (
  transactions
    .select('household_key')
    .distinct()
  )

# COMMAND ----------

# MAGIC %md We can now combine these to create the set of data on which we with to perform scoring:

# COMMAND ----------

# DBTITLE 1,Assemble Set to Score
scoring_df = (
  households
    .withColumn('commodity_desc', f.lit(commodity_desc))
    .withColumn('day', f.lit(last_day))
    )

display(scoring_df)

# COMMAND ----------

# MAGIC %md And to ensure we will recognize our model as the one associated with this commodity, we will assign it a name based on our commodity selection:

# COMMAND ----------

# MAGIC %md ## Step 2: Perform Scoring
# MAGIC 
# MAGIC Because we stored our model with metadata on how to lookup features in the feature store, batch scoring is pretty simple.  All we need to do is identify our model and the dataframe against wish we wish to score and let Databricks do the work.  
# MAGIC 
# MAGIC As simple as that sounds, it is essential that our scoring dataset contain valid entries in the fields that correspond to the lookup keys used to define the training set (in the last notebook).  But if we've done that, scoring looks like this:
# MAGIC 
# MAGIC **NOTE** Scores are returned as a field named *prediction*.  The dataframe generated by the *score_batch()* method call includes the features retrieved from the feature store. You will need to scroll to the far right of the dataframe to see the *prediction* column. 

# COMMAND ----------

# DBTITLE 1,Determine Model Name
model_name = 'propensity__'+commodity_desc.replace(' ','_')
model_name

# COMMAND ----------

# DBTITLE 1,Score the Set
# connect to feature store
fs = FeatureStoreClient()

# get scores
scores = fs.score_batch(
    model_uri = f'models:/{model_name}/production', # this is registered URI for a latest model (see prior notebook)
    df = scoring_df
  )

display(scores)

# COMMAND ----------

# MAGIC %md We can now persist our scored data for broader consumption.  As all we care about is the household, commodity, day and predicted score, we might define a table to house this data as follows:

# COMMAND ----------

# DBTITLE 1,Create Household Profile Table
# MAGIC %sql
# MAGIC 
# MAGIC create table if not exists household_profiles (
# MAGIC   household_key int,
# MAGIC   commodity_desc string,
# MAGIC   day date,
# MAGIC   prediction double
# MAGIC   )
# MAGIC   using delta;

# COMMAND ----------

# MAGIC %md We can then record our data to this table using a simple merge operation:

# COMMAND ----------

# DBTITLE 1,Persist Scores
# identify source and target objects for merge
target = DeltaTable.forName(spark, '{0}.household_profiles'.format(config['database']))
source = scores

# perform merge
(
  target.alias('target')
    .merge(
      source.alias('source'), 
      'target.household_key=source.household_key AND target.commodity_desc=source.commodity_desc AND target.day=source.day' # match on household_key and commodity and day
      )
    .whenMatchedUpdateAll()
    .whenNotMatchedInsertAll()
  ).execute()

# COMMAND ----------

# DBTITLE 1,Show Scored Results
display(
  (
    spark
      .table('{0}.household_profiles'.format(config['database']))
      .orderBy(['day','household_key'], ascending=[False,True])
    )
  )

# COMMAND ----------

# MAGIC %md ##Step 3: Next Steps
# MAGIC 
# MAGIC The notebooks in this accelerator have been purposefully written to show the propensity scoring exercise as comprised of three workflows.  In the first workflow, we generate features.  In the second workflow, we generate scores based on the latest features.  These two workflows are related but may be run on slightly different cadences in most real-world implementations.</p>
# MAGIC 
# MAGIC 
# MAGIC <img src='https://brysmiwasb.blob.core.windows.net/demos/images/propensity_workflow3.png' width='700'>
# MAGIC 
# MAGIC The final workflow is more of a manual workflow by which the Data Scientists in your organization periodically train and retrain various propensity scoring models.  Evaluation and review of these models often take place before these are elevated into production, and from that point, the scoring workflow picks up the trained model to generate its next set of scores.
# MAGIC 
# MAGIC In our demo, the scores that are generated are captured in a SQL table for persistance and review.  In most real-world implementations, these data would be published to one or more downstream systems supporting marketing activities (such as a CRM, CDP or MDP).  Databricks supports a variety of means for making data avaiable to such systems so please consult the documentation for the specific systems your organization uses to see which mechanisms are best suited for data integration. 

# COMMAND ----------

# MAGIC %md
# MAGIC 
# MAGIC &copy; 2022 Databricks, Inc. All rights reserved. The source in this notebook is provided subject to the [Databricks License](https://databricks.com/db-license-source).  All included or referenced third party libraries are subject to the licenses set forth below.
